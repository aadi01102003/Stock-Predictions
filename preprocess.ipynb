{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f98a804d-7185-43f5-b08d-bfe6c2994625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to 'merged_file.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "file1_path = 'combined_stock_data2_filtered.csv'\n",
    "file2_path = 'reshaped_engineered_features.csv'\n",
    "output_path = 'merged_file.csv'\n",
    "\n",
    "# Load the second dataset (df2) into memory\n",
    "df2 = pd.read_csv(file2_path)\n",
    "\n",
    "# Transform df2 into a suitable format for merging\n",
    "transformed_data = []\n",
    "for index, row in df2.iterrows():\n",
    "    date = row['Date']\n",
    "    indicator = row['Stock']\n",
    "    for stock_symbol, value in row.items():\n",
    "        if stock_symbol not in ['Date', 'Stock']:  # Skip the 'Date' and 'Stock' columns\n",
    "            transformed_data.append({\n",
    "                'Date': date,\n",
    "                'Stock': stock_symbol.upper(),  # Convert to uppercase to match df1\n",
    "                'Indicator': indicator,\n",
    "                'Value': value\n",
    "            })\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df2_transformed = pd.DataFrame(transformed_data)\n",
    "\n",
    "# Pivot the transformed df2\n",
    "df2_pivoted = df2_transformed.pivot_table(index=['Date', 'Stock'], columns='Indicator', values='Value').reset_index()\n",
    "\n",
    "# Define the chunk size for reading df1\n",
    "chunk_size = 10000  # Adjust based on your memory constraints\n",
    "\n",
    "# Initialize a flag to check if the output file is empty\n",
    "first_chunk = True\n",
    "\n",
    "# Read and process df1 in chunks\n",
    "for chunk in pd.read_csv(file1_path, chunksize=chunk_size):\n",
    "    # Merge the chunk with df2_pivoted\n",
    "    merged_chunk = pd.merge(chunk, df2_pivoted, on=['Date', 'Stock'], how='inner')\n",
    "\n",
    "    # Append the merged chunk to the output file\n",
    "    if first_chunk:\n",
    "        # Write header for the first chunk\n",
    "        merged_chunk.to_csv(output_path, index=False, mode='w')\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        # Append without header for subsequent chunks\n",
    "        merged_chunk.to_csv(output_path, index=False, mode='a', header=False)\n",
    "\n",
    "print(\"Merged data saved to 'merged_file.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e05407-e0d4-4daa-8e28-9db50ee1ac2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date       Open       High        Low      Close  Adj Close  \\\n",
      "0  2002-02-20  20.028612  21.316166  20.028612  20.643776  17.754581   \n",
      "1  2002-02-20  86.267700  88.622643  86.147552  87.973831  67.761444   \n",
      "2  2002-02-20   2.060000   2.190000   2.060000   2.190000   2.023804   \n",
      "3  2002-02-20   4.933333   4.933333   4.933333   4.933333   4.613485   \n",
      "4  2002-02-20   2.632428   2.719342   2.632428   2.714074   2.305438   \n",
      "\n",
      "      Volume Stock      Macd  Macd_Signal    Return        Rsi  Volatility  \n",
      "0  7024900.0     A -0.564911    -0.703869  0.107869  53.930432    0.586696  \n",
      "1  1514400.0    AA  0.345131    -0.203684  0.038581  57.389682    0.377003  \n",
      "2      300.0  AAME -0.068111    -0.075604  0.000000  51.213557    0.797065  \n",
      "3        0.0   AAN  0.220614     0.225360  0.000000  65.632980    0.502426  \n",
      "4    87000.0  AAON -0.007262    -0.006858  0.040909  48.335447    0.418702  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('merged_file.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b4e8a5b-15e7-43a0-b45d-1fa83e74d682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Stock</th>\n",
       "      <th>Macd</th>\n",
       "      <th>Macd_Signal</th>\n",
       "      <th>Return</th>\n",
       "      <th>Rsi</th>\n",
       "      <th>Volatility</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11356167</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>10.140000</td>\n",
       "      <td>10.340000</td>\n",
       "      <td>9.310000</td>\n",
       "      <td>9.470000</td>\n",
       "      <td>9.470000</td>\n",
       "      <td>34000.0</td>\n",
       "      <td>ZEUS</td>\n",
       "      <td>-0.825304</td>\n",
       "      <td>-0.973804</td>\n",
       "      <td>-0.085024</td>\n",
       "      <td>39.881108</td>\n",
       "      <td>0.971739</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11356168</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>25.870001</td>\n",
       "      <td>26.290001</td>\n",
       "      <td>25.020000</td>\n",
       "      <td>25.320000</td>\n",
       "      <td>25.320000</td>\n",
       "      <td>3837200.0</td>\n",
       "      <td>ZION</td>\n",
       "      <td>-3.627440</td>\n",
       "      <td>-4.124144</td>\n",
       "      <td>-0.053812</td>\n",
       "      <td>30.347153</td>\n",
       "      <td>0.861260</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11356169</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>4.110000</td>\n",
       "      <td>4.160000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>3.820000</td>\n",
       "      <td>3.820000</td>\n",
       "      <td>539500.0</td>\n",
       "      <td>ZIXI</td>\n",
       "      <td>-0.791701</td>\n",
       "      <td>-0.870836</td>\n",
       "      <td>-0.113689</td>\n",
       "      <td>35.274266</td>\n",
       "      <td>1.612495</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11356170</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>20.889999</td>\n",
       "      <td>21.190001</td>\n",
       "      <td>20.290001</td>\n",
       "      <td>20.389999</td>\n",
       "      <td>20.389999</td>\n",
       "      <td>33800.0</td>\n",
       "      <td>ZNH</td>\n",
       "      <td>-1.737241</td>\n",
       "      <td>-1.818122</td>\n",
       "      <td>-0.061234</td>\n",
       "      <td>36.502998</td>\n",
       "      <td>0.982866</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11356171</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>6.990000</td>\n",
       "      <td>6.990000</td>\n",
       "      <td>6.630000</td>\n",
       "      <td>6.740000</td>\n",
       "      <td>6.740000</td>\n",
       "      <td>193400.0</td>\n",
       "      <td>ZTR</td>\n",
       "      <td>-0.874450</td>\n",
       "      <td>-1.037236</td>\n",
       "      <td>-0.063889</td>\n",
       "      <td>38.583506</td>\n",
       "      <td>1.524556</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date       Open       High        Low      Close  Adj Close  \\\n",
       "11356167 2020-04-01  10.140000  10.340000   9.310000   9.470000   9.470000   \n",
       "11356168 2020-04-01  25.870001  26.290001  25.020000  25.320000  25.320000   \n",
       "11356169 2020-04-01   4.110000   4.160000   3.800000   3.820000   3.820000   \n",
       "11356170 2020-04-01  20.889999  21.190001  20.290001  20.389999  20.389999   \n",
       "11356171 2020-04-01   6.990000   6.990000   6.630000   6.740000   6.740000   \n",
       "\n",
       "             Volume Stock      Macd  Macd_Signal    Return        Rsi  \\\n",
       "11356167    34000.0  ZEUS -0.825304    -0.973804 -0.085024  39.881108   \n",
       "11356168  3837200.0  ZION -3.627440    -4.124144 -0.053812  30.347153   \n",
       "11356169   539500.0  ZIXI -0.791701    -0.870836 -0.113689  35.274266   \n",
       "11356170    33800.0   ZNH -1.737241    -1.818122 -0.061234  36.502998   \n",
       "11356171   193400.0   ZTR -0.874450    -1.037236 -0.063889  38.583506   \n",
       "\n",
       "          Volatility  Year  \n",
       "11356167    0.971739  2020  \n",
       "11356168    0.861260  2020  \n",
       "11356169    1.612495  2020  \n",
       "11356170    0.982866  2020  \n",
       "11356171    1.524556  2020  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56a1ee07-0700-49e0-96e2-c55225a0786d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in the DataFrame:\n",
      "[2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n"
     ]
    }
   ],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d', errors='coerce')  # Adjust the format if necessary\n",
    "\n",
    "# Extract the year from the 'Date' column\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# Get the unique years\n",
    "unique_years = df['Year'].unique()\n",
    "\n",
    "# Print the unique years\n",
    "print(\"Unique years in the DataFrame:\")\n",
    "print(sorted(unique_years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e23b3945-f3c5-4cc4-bbcc-b8caca50af3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original dataset...\n",
      "Filtering entries before 2002...\n",
      "Filtered dataset saved to 'combined_stock_data2_filtered.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the original dataset\n",
    "print(\"Loading original dataset...\")\n",
    "df = pd.read_csv('combined_stock_data2.csv', parse_dates=['Date'])\n",
    "\n",
    "# Step 2: Filter out entries before 2002\n",
    "print(\"Filtering entries before 2002...\")\n",
    "df = df[df['Date'] >= '2002-01-01']\n",
    "\n",
    "# Step 3: Save the filtered dataset to a new CSV file\n",
    "output_file = 'combined_stock_data2_filtered.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Filtered dataset saved to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fc044ee-1a19-4e6a-9a5e-9b0d065c9714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries for each year:\n",
      "    Year  Entries\n",
      "0   2002   517143\n",
      "1   2003   618061\n",
      "2   2004   630063\n",
      "3   2005   630155\n",
      "4   2006   627572\n",
      "5   2007   627685\n",
      "6   2008   632585\n",
      "7   2009   629438\n",
      "8   2010   629929\n",
      "9   2011   629806\n",
      "10  2012   625241\n",
      "11  2013   630194\n",
      "12  2014   629708\n",
      "13  2015   629611\n",
      "14  2016   630063\n",
      "15  2017   627516\n",
      "16  2018   625696\n",
      "17  2019   628464\n",
      "18  2020   157242\n"
     ]
    }
   ],
   "source": [
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# Group by 'Year' and count the number of entries for each year\n",
    "entries_per_year = df.groupby('Year').size().reset_index(name='Entries')\n",
    "\n",
    "# Print the result\n",
    "print(\"Number of entries for each year:\")\n",
    "print(entries_per_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f58000e-cb59-4919-a61d-393d37961902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values for each year:\n",
      "    Year  Macd  Rsi  Volatility  Return\n",
      "0   2002     0    0           0       0\n",
      "1   2003     0    0           0       0\n",
      "2   2004     0    0           0       0\n",
      "3   2005     0    0           0       0\n",
      "4   2006     0    0           0       0\n",
      "5   2007     0    0           0       0\n",
      "6   2008     0    0           0       0\n",
      "7   2009     0    0           0       0\n",
      "8   2010     0    0           0       0\n",
      "9   2011     0    0           0       0\n",
      "10  2012     0    0           0       0\n",
      "11  2013     0    0           0       0\n",
      "12  2014     0    0           0       0\n",
      "13  2015     0    0           0       0\n",
      "14  2016     0    0           0       0\n",
      "15  2017     0    0           0       0\n",
      "16  2018     0    0           0       0\n",
      "17  2019     0    0           0       0\n",
      "18  2020     0    0           0       0\n"
     ]
    }
   ],
   "source": [
    "columns_to_check = ['Macd', 'Rsi', 'Volatility', 'Return']\n",
    "\n",
    "# Group by 'Year' and count NaN values for each column\n",
    "nan_counts_per_year = df.groupby('Year')[columns_to_check].apply(lambda x: x.isna().sum()).reset_index()\n",
    "\n",
    "# Print the result\n",
    "print(\"Number of NaN values for each year:\")\n",
    "print(nan_counts_per_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07bc6cea-6962-4b2b-9cd7-3c53b008b79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 1it [00:11, 11.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineered features saved to 'engineered_features.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import talib  # Import TA-Lib\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Function to calculate RSI using TA-Lib\n",
    "def calculate_rsi(prices, period=14):\n",
    "    \"\"\"\n",
    "    Calculate the Relative Strength Index (RSI) using TA-Lib.\n",
    "    \"\"\"\n",
    "    return talib.RSI(prices, timeperiod=period)\n",
    "\n",
    "# Function to calculate MACD using TA-Lib\n",
    "def calculate_macd(prices, short_period=12, long_period=26, signal_period=9):\n",
    "    \"\"\"\n",
    "    Calculate the MACD and Signal Line using TA-Lib.\n",
    "    \"\"\"\n",
    "    macd, signal, _ = talib.MACD(prices, fastperiod=short_period, slowperiod=long_period, signalperiod=signal_period)\n",
    "    return macd, signal\n",
    "\n",
    "# Step 1: Load the cleaned dataset in chunks\n",
    "chunk_size = 10000  # Adjust based on your system's memory\n",
    "chunks = pd.read_csv('cleaned_adjusted_close_prices.csv', parse_dates=['Date'], chunksize=chunk_size)\n",
    "\n",
    "# Initialize a list to store processed chunks\n",
    "processed_chunks = []\n",
    "\n",
    "# Process each chunk\n",
    "for i, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks\")):\n",
    "    # Set 'Date' as the index\n",
    "    chunk.set_index('Date', inplace=True)\n",
    "\n",
    "    # Feature Engineering\n",
    "    all_features = {}\n",
    "    for stock in chunk.columns:\n",
    "        prices = chunk[stock]\n",
    "        returns = prices.pct_change().dropna()\n",
    "        volatility = returns.rolling(window=21).std() * np.sqrt(252)\n",
    "        rsi = calculate_rsi(prices, period=14)\n",
    "        macd, signal = calculate_macd(prices, short_period=12, long_period=26, signal_period=9)\n",
    "        stock_features = pd.DataFrame({\n",
    "            f'{stock}_Return': returns,\n",
    "            f'{stock}_Volatility': volatility,\n",
    "            f'{stock}_RSI': rsi,\n",
    "            f'{stock}_MACD': macd,\n",
    "            f'{stock}_MACD_Signal': signal\n",
    "        })\n",
    "        all_features[stock] = stock_features\n",
    "\n",
    "    # Skip this chunk if no features were computed\n",
    "    if not all_features:\n",
    "        print(f\"Chunk {i + 1} has no features computed. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Combine features for all stocks into a single DataFrame\n",
    "    features = pd.concat(all_features.values(), axis=1).dropna()\n",
    "\n",
    "    # Skip this chunk if no features are available after concatenation\n",
    "    if features.empty:\n",
    "        print(f\"Chunk {i + 1} has no features after concatenation. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Append the processed chunk to the list\n",
    "    processed_chunks.append(features)\n",
    "\n",
    "# Combine all processed chunks into a single DataFrame\n",
    "if processed_chunks:\n",
    "    final_features = pd.concat(processed_chunks)\n",
    "    # Save the final features to a new CSV file\n",
    "    final_features.to_csv('engineered_features.csv')\n",
    "    print(\"\\nEngineered features saved to 'engineered_features.csv'.\")\n",
    "else:\n",
    "    print(\"No features were computed. Check the input dataset and filtering conditions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "631e28fc-2e4e-4ae1-9482-594d3dbd3f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reshaped engineered features...\n",
      "Processing chunk 1...\n",
      "Processing chunk 2...\n",
      "Processing chunk 3...\n",
      "Processing chunk 4...\n",
      "Processing chunk 5...\n",
      "Processing chunk 6...\n",
      "Processing chunk 7...\n",
      "Processing chunk 8...\n",
      "Processing chunk 9...\n",
      "Processing chunk 10...\n",
      "Processing chunk 11...\n",
      "Processing chunk 12...\n",
      "Processing chunk 13...\n",
      "Processing chunk 14...\n",
      "Processing chunk 15...\n",
      "Processing chunk 16...\n",
      "Processing chunk 17...\n",
      "Processing chunk 18...\n",
      "Processing chunk 19...\n",
      "Processing chunk 20...\n",
      "Processing chunk 21...\n",
      "Processing chunk 22...\n",
      "Processing chunk 23...\n",
      "Processing chunk 24...\n",
      "Processing chunk 25...\n",
      "Processing chunk 26...\n",
      "Processing chunk 27...\n",
      "Processing chunk 28...\n",
      "Processing chunk 29...\n",
      "Processing chunk 30...\n",
      "Processing chunk 31...\n",
      "Processing chunk 32...\n",
      "Processing chunk 33...\n",
      "Processing chunk 34...\n",
      "Processing chunk 35...\n",
      "Processing chunk 36...\n",
      "Processing chunk 37...\n",
      "Processing chunk 38...\n",
      "Processing chunk 39...\n",
      "Processing chunk 40...\n",
      "Processing chunk 41...\n",
      "Processing chunk 42...\n",
      "Processing chunk 43...\n",
      "Processing chunk 44...\n",
      "Processing chunk 45...\n",
      "Processing chunk 46...\n",
      "Processing chunk 47...\n",
      "Processing chunk 48...\n",
      "Processing chunk 49...\n",
      "Processing chunk 50...\n",
      "Processing chunk 51...\n",
      "Processing chunk 52...\n",
      "Processing chunk 53...\n",
      "Processing chunk 54...\n",
      "Processing chunk 55...\n",
      "Processing chunk 56...\n",
      "Processing chunk 57...\n",
      "Processing chunk 58...\n",
      "Processing chunk 59...\n",
      "Processing chunk 60...\n",
      "Processing chunk 61...\n",
      "Processing chunk 62...\n",
      "Processing chunk 63...\n",
      "Processing chunk 64...\n",
      "Processing chunk 65...\n",
      "Processing chunk 66...\n",
      "Processing chunk 67...\n",
      "Processing chunk 68...\n",
      "Processing chunk 69...\n",
      "Processing chunk 70...\n",
      "Processing chunk 71...\n",
      "Processing chunk 72...\n",
      "Processing chunk 73...\n",
      "Processing chunk 74...\n",
      "Processing chunk 75...\n",
      "Processing chunk 76...\n",
      "Processing chunk 77...\n",
      "Processing chunk 78...\n",
      "Processing chunk 79...\n",
      "Processing chunk 80...\n",
      "Processing chunk 81...\n",
      "Processing chunk 82...\n",
      "Processing chunk 83...\n",
      "Processing chunk 84...\n",
      "Processing chunk 85...\n",
      "Processing chunk 86...\n",
      "Processing chunk 87...\n",
      "Processing chunk 88...\n",
      "Processing chunk 89...\n",
      "Processing chunk 90...\n",
      "Processing chunk 91...\n",
      "Processing chunk 92...\n",
      "Processing chunk 93...\n",
      "Processing chunk 94...\n",
      "Processing chunk 95...\n",
      "Processing chunk 96...\n",
      "Processing chunk 97...\n",
      "Processing chunk 98...\n",
      "Processing chunk 99...\n",
      "Processing chunk 100...\n",
      "Processing chunk 101...\n",
      "Processing chunk 102...\n",
      "Processing chunk 103...\n",
      "Processing chunk 104...\n",
      "Processing chunk 105...\n",
      "Processing chunk 106...\n",
      "Processing chunk 107...\n",
      "Processing chunk 108...\n",
      "Processing chunk 109...\n",
      "Processing chunk 110...\n",
      "Processing chunk 111...\n",
      "Processing chunk 112...\n",
      "Processing chunk 113...\n",
      "Processing chunk 114...\n",
      "Processing chunk 115...\n",
      "Processing chunk 116...\n",
      "Processing chunk 117...\n",
      "Processing chunk 118...\n",
      "Processing chunk 119...\n",
      "Processing chunk 120...\n",
      "Processing chunk 121...\n",
      "Processing chunk 122...\n",
      "Processing chunk 123...\n",
      "Processing chunk 124...\n",
      "Processing chunk 125...\n",
      "Processing chunk 126...\n",
      "Processing chunk 127...\n",
      "Processing chunk 128...\n",
      "Processing chunk 129...\n",
      "Processing chunk 130...\n",
      "Processing chunk 131...\n",
      "Processing chunk 132...\n",
      "Processing chunk 133...\n",
      "Processing chunk 134...\n",
      "Processing chunk 135...\n",
      "Processing chunk 136...\n",
      "Processing chunk 137...\n",
      "Processing chunk 138...\n",
      "Processing chunk 139...\n",
      "Processing chunk 140...\n",
      "Processing chunk 141...\n",
      "Processing chunk 142...\n",
      "Processing chunk 143...\n",
      "Processing chunk 144...\n",
      "Processing chunk 145...\n",
      "Processing chunk 146...\n",
      "Processing chunk 147...\n",
      "Processing chunk 148...\n",
      "Processing chunk 149...\n",
      "Processing chunk 150...\n",
      "Processing chunk 151...\n",
      "Processing chunk 152...\n",
      "Processing chunk 153...\n",
      "Processing chunk 154...\n",
      "Processing chunk 155...\n",
      "Processing chunk 156...\n",
      "Processing chunk 157...\n",
      "Processing chunk 158...\n",
      "Processing chunk 159...\n",
      "Processing chunk 160...\n",
      "Processing chunk 161...\n",
      "Processing chunk 162...\n",
      "Processing chunk 163...\n",
      "Processing chunk 164...\n",
      "Processing chunk 165...\n",
      "Processing chunk 166...\n",
      "Processing chunk 167...\n",
      "Processing chunk 168...\n",
      "Processing chunk 169...\n",
      "Processing chunk 170...\n",
      "Processing chunk 171...\n",
      "Processing chunk 172...\n",
      "Processing chunk 173...\n",
      "Processing chunk 174...\n",
      "Processing chunk 175...\n",
      "Processing chunk 176...\n",
      "Processing chunk 177...\n",
      "Processing chunk 178...\n",
      "Processing chunk 179...\n",
      "Processing chunk 180...\n",
      "Processing chunk 181...\n",
      "Processing chunk 182...\n",
      "Processing chunk 183...\n",
      "Processing chunk 184...\n",
      "Processing chunk 185...\n",
      "Processing chunk 186...\n",
      "Processing chunk 187...\n",
      "Processing chunk 188...\n",
      "Processing chunk 189...\n",
      "Processing chunk 190...\n",
      "Processing chunk 191...\n",
      "Processing chunk 192...\n",
      "Processing chunk 193...\n",
      "Processing chunk 194...\n",
      "Processing chunk 195...\n",
      "Processing chunk 196...\n",
      "Processing chunk 197...\n",
      "Processing chunk 198...\n",
      "Processing chunk 199...\n",
      "Processing chunk 200...\n",
      "Processing chunk 201...\n",
      "Processing chunk 202...\n",
      "Processing chunk 203...\n",
      "Processing chunk 204...\n",
      "Processing chunk 205...\n",
      "Processing chunk 206...\n",
      "Processing chunk 207...\n",
      "Processing chunk 208...\n",
      "Processing chunk 209...\n",
      "Processing chunk 210...\n",
      "Processing chunk 211...\n",
      "Processing chunk 212...\n",
      "Processing chunk 213...\n",
      "Processing chunk 214...\n",
      "Processing chunk 215...\n",
      "Processing chunk 216...\n",
      "Processing chunk 217...\n",
      "Processing chunk 218...\n",
      "Processing chunk 219...\n",
      "Processing chunk 220...\n",
      "Processing chunk 221...\n",
      "Processing chunk 222...\n",
      "Processing chunk 223...\n",
      "Processing chunk 224...\n",
      "Processing chunk 225...\n",
      "Processing chunk 226...\n",
      "Processing chunk 227...\n",
      "Processing chunk 228...\n",
      "Processing chunk 229...\n",
      "Processing chunk 230...\n",
      "Processing chunk 231...\n",
      "Processing chunk 232...\n",
      "Processing chunk 233...\n",
      "Processing chunk 234...\n",
      "Processing chunk 235...\n",
      "Processing chunk 236...\n",
      "Processing chunk 237...\n",
      "Processing chunk 238...\n",
      "Processing chunk 239...\n",
      "Processing chunk 240...\n",
      "Processing chunk 241...\n",
      "Processing chunk 242...\n",
      "Processing chunk 243...\n",
      "Processing chunk 244...\n",
      "Processing chunk 245...\n",
      "Processing chunk 246...\n",
      "Processing chunk 247...\n",
      "Processing chunk 248...\n",
      "Processing chunk 249...\n",
      "Processing chunk 250...\n",
      "Processing chunk 251...\n",
      "Processing chunk 252...\n",
      "Processing chunk 253...\n",
      "Processing chunk 254...\n",
      "Processing chunk 255...\n",
      "Processing chunk 256...\n",
      "Processing chunk 257...\n",
      "Processing chunk 258...\n",
      "Processing chunk 259...\n",
      "Processing chunk 260...\n",
      "Processing chunk 261...\n",
      "Processing chunk 262...\n",
      "Processing chunk 263...\n",
      "Processing chunk 264...\n",
      "Processing chunk 265...\n",
      "Processing chunk 266...\n",
      "Processing chunk 267...\n",
      "Processing chunk 268...\n",
      "Processing chunk 269...\n",
      "Processing chunk 270...\n",
      "Processing chunk 271...\n",
      "Processing chunk 272...\n",
      "Processing chunk 273...\n",
      "Processing chunk 274...\n",
      "Processing chunk 275...\n",
      "Processing chunk 276...\n",
      "Processing chunk 277...\n",
      "Processing chunk 278...\n",
      "Processing chunk 279...\n",
      "Processing chunk 280...\n",
      "Processing chunk 281...\n",
      "Processing chunk 282...\n",
      "Processing chunk 283...\n",
      "Processing chunk 284...\n",
      "Processing chunk 285...\n",
      "Processing chunk 286...\n",
      "Processing chunk 287...\n",
      "Processing chunk 288...\n",
      "Processing chunk 289...\n",
      "Processing chunk 290...\n",
      "Processing chunk 291...\n",
      "Processing chunk 292...\n",
      "Processing chunk 293...\n",
      "Processing chunk 294...\n",
      "Processing chunk 295...\n",
      "Processing chunk 296...\n",
      "Processing chunk 297...\n",
      "Processing chunk 298...\n",
      "Processing chunk 299...\n",
      "Processing chunk 300...\n",
      "Processing chunk 301...\n",
      "Processing chunk 302...\n",
      "Processing chunk 303...\n",
      "Processing chunk 304...\n",
      "Processing chunk 305...\n",
      "Processing chunk 306...\n",
      "Processing chunk 307...\n",
      "Processing chunk 308...\n",
      "Processing chunk 309...\n",
      "Processing chunk 310...\n",
      "Processing chunk 311...\n",
      "Processing chunk 312...\n",
      "Processing chunk 313...\n",
      "Processing chunk 314...\n",
      "Processing chunk 315...\n",
      "Processing chunk 316...\n",
      "Processing chunk 317...\n",
      "Processing chunk 318...\n",
      "Processing chunk 319...\n",
      "Processing chunk 320...\n",
      "Processing chunk 321...\n",
      "Processing chunk 322...\n",
      "Processing chunk 323...\n",
      "Processing chunk 324...\n",
      "Processing chunk 325...\n",
      "Processing chunk 326...\n",
      "Processing chunk 327...\n",
      "Processing chunk 328...\n",
      "Processing chunk 329...\n",
      "Processing chunk 330...\n",
      "Processing chunk 331...\n",
      "Processing chunk 332...\n",
      "Processing chunk 333...\n",
      "Processing chunk 334...\n",
      "Processing chunk 335...\n",
      "Processing chunk 336...\n",
      "Processing chunk 337...\n",
      "Processing chunk 338...\n",
      "Processing chunk 339...\n",
      "Processing chunk 340...\n",
      "Processing chunk 341...\n",
      "Processing chunk 342...\n",
      "Processing chunk 343...\n",
      "Processing chunk 344...\n",
      "Processing chunk 345...\n",
      "Processing chunk 346...\n",
      "Processing chunk 347...\n",
      "Processing chunk 348...\n",
      "Processing chunk 349...\n",
      "Processing chunk 350...\n",
      "Processing chunk 351...\n",
      "Processing chunk 352...\n",
      "Processing chunk 353...\n",
      "Processing chunk 354...\n",
      "Processing chunk 355...\n",
      "Processing chunk 356...\n",
      "Processing chunk 357...\n",
      "Processing chunk 358...\n",
      "Processing chunk 359...\n",
      "Processing chunk 360...\n",
      "Processing chunk 361...\n",
      "Processing chunk 362...\n",
      "Processing chunk 363...\n",
      "Processing chunk 364...\n",
      "Processing chunk 365...\n",
      "Processing chunk 366...\n",
      "Processing chunk 367...\n",
      "Processing chunk 368...\n",
      "Processing chunk 369...\n",
      "Processing chunk 370...\n",
      "Processing chunk 371...\n",
      "Processing chunk 372...\n",
      "Processing chunk 373...\n",
      "Processing chunk 374...\n",
      "Processing chunk 375...\n",
      "Processing chunk 376...\n",
      "Processing chunk 377...\n",
      "Processing chunk 378...\n",
      "Processing chunk 379...\n",
      "Processing chunk 380...\n",
      "Processing chunk 381...\n",
      "Processing chunk 382...\n",
      "Processing chunk 383...\n",
      "Processing chunk 384...\n",
      "Processing chunk 385...\n",
      "Processing chunk 386...\n",
      "Processing chunk 387...\n",
      "Processing chunk 388...\n",
      "Processing chunk 389...\n",
      "Processing chunk 390...\n",
      "Processing chunk 391...\n",
      "Processing chunk 392...\n",
      "Processing chunk 393...\n",
      "Processing chunk 394...\n",
      "Processing chunk 395...\n",
      "Processing chunk 396...\n",
      "Processing chunk 397...\n",
      "Processing chunk 398...\n",
      "Processing chunk 399...\n",
      "Processing chunk 400...\n",
      "Processing chunk 401...\n",
      "Processing chunk 402...\n",
      "Processing chunk 403...\n",
      "Processing chunk 404...\n",
      "Processing chunk 405...\n",
      "Processing chunk 406...\n",
      "Processing chunk 407...\n",
      "Processing chunk 408...\n",
      "Processing chunk 409...\n",
      "Processing chunk 410...\n",
      "Processing chunk 411...\n",
      "Processing chunk 412...\n",
      "Processing chunk 413...\n",
      "Processing chunk 414...\n",
      "Processing chunk 415...\n",
      "Processing chunk 416...\n",
      "Processing chunk 417...\n",
      "Processing chunk 418...\n",
      "Processing chunk 419...\n",
      "Processing chunk 420...\n",
      "Processing chunk 421...\n",
      "Processing chunk 422...\n",
      "Processing chunk 423...\n",
      "Processing chunk 424...\n",
      "Processing chunk 425...\n",
      "Processing chunk 426...\n",
      "Processing chunk 427...\n",
      "Processing chunk 428...\n",
      "Processing chunk 429...\n",
      "Processing chunk 430...\n",
      "Processing chunk 431...\n",
      "Processing chunk 432...\n",
      "Processing chunk 433...\n",
      "Processing chunk 434...\n",
      "Processing chunk 435...\n",
      "Processing chunk 436...\n",
      "Processing chunk 437...\n",
      "Processing chunk 438...\n",
      "Processing chunk 439...\n",
      "Processing chunk 440...\n",
      "Processing chunk 441...\n",
      "Processing chunk 442...\n",
      "Processing chunk 443...\n",
      "Processing chunk 444...\n",
      "Processing chunk 445...\n",
      "Processing chunk 446...\n",
      "Processing chunk 447...\n",
      "Processing chunk 448...\n",
      "Processing chunk 449...\n",
      "Processing chunk 450...\n",
      "Processing chunk 451...\n",
      "Processing chunk 452...\n",
      "Processing chunk 453...\n",
      "Processing chunk 454...\n",
      "Processing chunk 455...\n",
      "Processing chunk 456...\n",
      "Processing chunk 457...\n",
      "Processing chunk 458...\n",
      "Processing chunk 459...\n",
      "Processing chunk 460...\n",
      "Processing chunk 461...\n",
      "Processing chunk 462...\n",
      "Processing chunk 463...\n",
      "Processing chunk 464...\n",
      "Processing chunk 465...\n",
      "Processing chunk 466...\n",
      "Processing chunk 467...\n",
      "Processing chunk 468...\n",
      "Processing chunk 469...\n",
      "Processing chunk 470...\n",
      "Processing chunk 471...\n",
      "Processing chunk 472...\n",
      "Processing chunk 473...\n",
      "Processing chunk 474...\n",
      "Processing chunk 475...\n",
      "Processing chunk 476...\n",
      "Processing chunk 477...\n",
      "Processing chunk 478...\n",
      "Processing chunk 479...\n",
      "Processing chunk 480...\n",
      "Processing chunk 481...\n",
      "Processing chunk 482...\n",
      "Processing chunk 483...\n",
      "Processing chunk 484...\n",
      "\n",
      "Final dataset saved to 'original_with_engineered_features_chunked.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Load the reshaped engineered features\n",
    "print(\"Loading reshaped engineered features...\")\n",
    "reshaped_features = pd.read_csv('reshaped_engineered_features.csv', parse_dates=['Date'])\n",
    "\n",
    "# Step 5: Load the original dataset in chunks\n",
    "chunk_size = 10000  # Adjust based on your system's memory\n",
    "chunks = pd.read_csv('combined_stock_data2.csv', parse_dates=['Date'], chunksize=chunk_size)\n",
    "\n",
    "# Step 6: Initialize the output CSV file\n",
    "output_file = 'original_with_engineered_features_chunked.csv'\n",
    "header = True  # Write header only for the first chunk\n",
    "\n",
    "# Step 7: Process each chunk and write to CSV\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i + 1}...\")\n",
    "    \n",
    "    # Ensure the column names are consistent\n",
    "    chunk.columns = chunk.columns.str.strip().str.title()\n",
    "    \n",
    "    # Merge the chunk with the reshaped engineered features on 'Date' and 'Stock'\n",
    "    merged_chunk = pd.merge(\n",
    "        chunk,  # Original chunk\n",
    "        reshaped_features,  # Reshaped engineered features\n",
    "        on=['Date', 'Stock'],  # Merge on Date and Stock\n",
    "        how='inner'  # Keep only rows with matching Date and Stock\n",
    "    )\n",
    "    \n",
    "    # Write the merged chunk to the output CSV file\n",
    "    merged_chunk.to_csv(output_file, mode='a', index=False, header=header)\n",
    "    header = False  # Do not write header for subsequent chunks\n",
    "\n",
    "print(\"\\nFinal dataset saved to 'original_with_engineered_features_chunked.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0990ee-9ddb-4858-9201-69da817821c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(prices, period=14):\n",
    "    \"\"\"\n",
    "    Calculate the Relative Strength Index (RSI) using TA-Lib.\n",
    "    \"\"\"\n",
    "    return talib.RSI(prices, timeperiod=period)\n",
    "\n",
    "# Function to calculate MACD using TA-Lib\n",
    "def calculate_macd(prices, short_period=12, long_period=26, signal_period=9):\n",
    "    \"\"\"\n",
    "    Calculate the MACD and Signal Line using TA-Lib.\n",
    "    \"\"\"\n",
    "    macd, signal, _ = talib.MACD(prices, fastperiod=short_period, slowperiod=long_period, signalperiod=signal_period)\n",
    "    return macd, signal\n",
    "\n",
    "# Initialize a dictionary to store features for all stocks\n",
    "all_features = {}\n",
    "for stock in adjusted_close_prices.columns:\n",
    "    prices = adjusted_close_prices[stock]\n",
    "    \n",
    "    # Calculate daily returns\n",
    "    returns = prices.pct_change().dropna()\n",
    "    \n",
    "    # Calculate rolling volatility (annualized)\n",
    "    volatility = returns.rolling(window=21).std() * np.sqrt(252)\n",
    "    \n",
    "    # Calculate RSI using TA-Lib\n",
    "    rsi = calculate_rsi(prices, period=14)\n",
    "    \n",
    "    # Calculate MACD and Signal Line using TA-Lib\n",
    "    macd, signal = calculate_macd(prices, short_period=12, long_period=26, signal_period=9)\n",
    "    \n",
    "    # Combine features for this stock into a DataFrame\n",
    "    stock_features = pd.DataFrame({\n",
    "        f'{stock}_Return': returns,\n",
    "        f'{stock}_Volatility': volatility,\n",
    "        f'{stock}_RSI': rsi,\n",
    "        f'{stock}_MACD': macd,\n",
    "        f'{stock}_MACD_Signal': signal\n",
    "    })\n",
    "    \n",
    "    # Add the stock's features to the dictionary\n",
    "    all_features[stock] = stock_features\n",
    "\n",
    "# Combine features for all stocks into a single DataFrame\n",
    "features = pd.concat(all_features.values(), axis=1).dropna()\n",
    "\n",
    "# Display the features DataFrame\n",
    "print(\"\\nEngineered Features Shape:\", features.shape)\n",
    "print(\"\\nEngineered Features Preview:\")\n",
    "print(features.head())\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Step 8: Merge the original dataset with the new features\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "# Reset the index of the original dataset to merge on 'Date'\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Merge the original dataset with the features DataFrame on 'Date'\n",
    "merged_df = pd.merge(df, features, left_on='Date', right_index=True, how='inner')\n",
    "\n",
    "# Display the merged dataset shape and preview\n",
    "print(\"\\nMerged Dataset Shape:\", merged_df.shape)\n",
    "print(\"\\nMerged Dataset Preview:\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Step 9: Save the merged dataset to a new CSV file\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "merged_df.to_csv('original_with_new_features.csv', index=False)\n",
    "print(\"\\nMerged dataset saved to 'original_with_new_features.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "481fa7d3-99f3-4aa3-abd5-6582a04b1da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "        Date      Open      High       Low     Close  Adj Close    Volume  \\\n",
      "0 1962-01-02  6.532155  6.556185  6.532155  6.532155   1.536658   55900.0   \n",
      "1 1962-01-02  6.125844  6.160982  6.125844  6.125844   1.414651   59700.0   \n",
      "2 1962-01-02  0.837449  0.837449  0.823045  0.823045   0.145748  352200.0   \n",
      "3 1962-01-02  1.604167  1.619792  1.588542  1.604167   0.136957  163200.0   \n",
      "4 1962-01-02  0.000000  3.296131  3.244048  3.296131   0.051993  105600.0   \n",
      "\n",
      "  Stock  \n",
      "0    AA  \n",
      "1  ARNC  \n",
      "2    BA  \n",
      "3   CAT  \n",
      "4   CVX  \n",
      "\n",
      "Pivoted Dataset Shape: (14717, 5884)\n",
      "\n",
      "Dataset Shape After Restricting Time Range: (4617, 5884)\n",
      "\n",
      "Dataset Shape After Dropping Rows with Excessive Missing Data: (4594, 5884)\n",
      "\n",
      "Dataset Shape After Dropping Stocks with Excessive Missing Data: (4594, 2501)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_3448\\3548282476.py:46: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  adjusted_close_prices.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_3448\\3548282476.py:47: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  adjusted_close_prices.fillna(method='bfill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Dataset Shape: (4594, 2501)\n",
      "\n",
      "Final Dataset Preview:\n",
      "Stock               A         AA      AAME       AAN      AAON        AAP  \\\n",
      "Date                                                                        \n",
      "2002-01-02  17.994514  65.743896  2.051527  3.740664  2.460924  15.107107   \n",
      "2002-01-03  19.132622  66.591476  2.125456  3.823791  2.288659  14.322977   \n",
      "2002-01-04  20.166155  68.728867  2.097733  3.823791  2.349064  13.437671   \n",
      "2002-01-07  20.086178  70.313538  2.116215  3.823791  2.347945  13.216344   \n",
      "2002-01-08  20.147697  68.802628  2.310278  4.161834  2.349064  12.995018   \n",
      "\n",
      "Stock           AAPL      AAXN         AB       ABB  ...       YUM       YUMA  \\\n",
      "Date                                                 ...                        \n",
      "2002-01-02  1.444668  1.115833  13.765452  5.096856  ...  5.662119  95.979019   \n",
      "2002-01-03  1.462029  1.120833  13.919203  5.467911  ...  5.781286  95.979019   \n",
      "2002-01-04  1.468849  1.100833  14.249454  5.725589  ...  6.005459  95.979019   \n",
      "2002-01-07  1.419867  1.090000  14.320631  5.514292  ...  6.117543  95.979019   \n",
      "2002-01-08  1.401886  1.216667  14.272227  5.581290  ...  6.235531  95.979019   \n",
      "\n",
      "Stock         YVR        ZBH       ZBRA      ZEUS       ZION  ZIXI       ZNH  \\\n",
      "Date                                                                           \n",
      "2002-01-02  24.00  28.107138  24.813334  2.317158  39.657085  5.49  8.200848   \n",
      "2002-01-03  23.00  27.930490  25.435556  2.289683  40.310600  6.02  8.217381   \n",
      "2002-01-04  23.00  27.474894  26.133333  2.198095  40.648907  6.15  8.300053   \n",
      "2002-01-07  23.75  27.474894  25.680000  2.564445  40.625832  5.96  8.900784   \n",
      "2002-01-08  25.25  28.358187  25.799999  2.747620  40.556641  6.08  9.314135   \n",
      "\n",
      "Stock            ZTR  \n",
      "Date                  \n",
      "2002-01-02  3.693760  \n",
      "2002-01-03  3.743570  \n",
      "2002-01-04  3.775032  \n",
      "2002-01-07  3.780276  \n",
      "2002-01-08  3.780276  \n",
      "\n",
      "[5 rows x 2501 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "df = pd.read_csv('combined_stock_data2.csv', parse_dates=['Date'])\n",
    "\n",
    "# Ensure the column names are consistent\n",
    "df.columns = df.columns.str.strip().str.title()\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 2: Pivot the data to get adjusted close prices per stock\n",
    "adjusted_close_prices = df.pivot(index='Date', columns='Stock', values='Adj Close')\n",
    "\n",
    "# Display the shape of the pivoted data\n",
    "print(\"\\nPivoted Dataset Shape:\", adjusted_close_prices.shape)\n",
    "\n",
    "# Step 3: Restrict the time range\n",
    "# Focus on recent years (e.g., 2002 onwards)\n",
    "start_date = \"2002-01-01\"\n",
    "adjusted_close_prices = adjusted_close_prices.loc[start_date:]\n",
    "\n",
    "# Display the shape after restricting the time range\n",
    "print(\"\\nDataset Shape After Restricting Time Range:\", adjusted_close_prices.shape)\n",
    "\n",
    "# Step 4: Drop rows (dates) with excessive missing data\n",
    "# Define a threshold for acceptable missing data per row (e.g., 70% valid data required)\n",
    "row_threshold = 0.7 * adjusted_close_prices.shape[1]  # Allow up to 30% missing data per row\n",
    "adjusted_close_prices = adjusted_close_prices.loc[adjusted_close_prices.isna().sum(axis=1) < row_threshold]\n",
    "\n",
    "# Display the shape after dropping rows with excessive missing data\n",
    "print(\"\\nDataset Shape After Dropping Rows with Excessive Missing Data:\", adjusted_close_prices.shape)\n",
    "\n",
    "# Step 5: Drop stocks (columns) with excessive missing data\n",
    "# Define a threshold for acceptable missing data per stock (e.g., 10% missing data allowed)\n",
    "stock_threshold = 0.1 * adjusted_close_prices.shape[0]  # Allow up to 10% missing data per stock\n",
    "adjusted_close_prices = adjusted_close_prices.loc[:, adjusted_close_prices.isna().sum() < stock_threshold]\n",
    "\n",
    "# Display the shape after dropping stocks with excessive missing data\n",
    "print(\"\\nDataset Shape After Dropping Stocks with Excessive Missing Data:\", adjusted_close_prices.shape)\n",
    "\n",
    "# Step 6: Fill remaining missing values\n",
    "# Use forward-fill and backward-fill to handle remaining NaNs\n",
    "adjusted_close_prices.fillna(method='ffill', inplace=True)\n",
    "adjusted_close_prices.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Display the final dataset shape and preview\n",
    "print(\"\\nFinal Dataset Shape:\", adjusted_close_prices.shape)\n",
    "print(\"\\nFinal Dataset Preview:\")\n",
    "print(adjusted_close_prices.head())\n",
    "\n",
    "# Optional: Save the cleaned dataset to a new file\n",
    "adjusted_close_prices.to_csv('cleaned_adjusted_close_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8894d59-cae1-499c-beef-2e926980cece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1962-01-02</td>\n",
       "      <td>6.532155</td>\n",
       "      <td>6.556185</td>\n",
       "      <td>6.532155</td>\n",
       "      <td>6.532155</td>\n",
       "      <td>1.536658</td>\n",
       "      <td>55900.0</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1962-01-02</td>\n",
       "      <td>6.125844</td>\n",
       "      <td>6.160982</td>\n",
       "      <td>6.125844</td>\n",
       "      <td>6.125844</td>\n",
       "      <td>1.414651</td>\n",
       "      <td>59700.0</td>\n",
       "      <td>ARNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1962-01-02</td>\n",
       "      <td>0.837449</td>\n",
       "      <td>0.837449</td>\n",
       "      <td>0.823045</td>\n",
       "      <td>0.823045</td>\n",
       "      <td>0.145748</td>\n",
       "      <td>352200.0</td>\n",
       "      <td>BA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1962-01-02</td>\n",
       "      <td>1.604167</td>\n",
       "      <td>1.619792</td>\n",
       "      <td>1.588542</td>\n",
       "      <td>1.604167</td>\n",
       "      <td>0.136957</td>\n",
       "      <td>163200.0</td>\n",
       "      <td>CAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1962-01-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.296131</td>\n",
       "      <td>3.244048</td>\n",
       "      <td>3.296131</td>\n",
       "      <td>0.051993</td>\n",
       "      <td>105600.0</td>\n",
       "      <td>CVX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date      Open      High       Low     Close  Adj Close    Volume  \\\n",
       "0 1962-01-02  6.532155  6.556185  6.532155  6.532155   1.536658   55900.0   \n",
       "1 1962-01-02  6.125844  6.160982  6.125844  6.125844   1.414651   59700.0   \n",
       "2 1962-01-02  0.837449  0.837449  0.823045  0.823045   0.145748  352200.0   \n",
       "3 1962-01-02  1.604167  1.619792  1.588542  1.604167   0.136957  163200.0   \n",
       "4 1962-01-02  0.000000  3.296131  3.244048  3.296131   0.051993  105600.0   \n",
       "\n",
       "  Stock  \n",
       "0    AA  \n",
       "1  ARNC  \n",
       "2    BA  \n",
       "3   CAT  \n",
       "4   CVX  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc6706ac-1256-4869-9f6e-49c2923a1703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineered Features Shape: (4561, 12505)\n",
      "\n",
      "Engineered Features Preview:\n",
      "            A_Return  A_Volatility      A_RSI    A_MACD  A_MACD_Signal  \\\n",
      "Date                                                                     \n",
      "2002-02-20  0.107869      0.586696  53.930432 -0.564911      -0.703869   \n",
      "2002-02-21 -0.024255      0.568822  50.728646 -0.492460      -0.661587   \n",
      "2002-02-22  0.003551      0.559301  51.174603 -0.425176      -0.614305   \n",
      "2002-02-25  0.026185      0.558100  54.459439 -0.331299      -0.557704   \n",
      "2002-02-26  0.024483      0.563412  57.419435 -0.219130      -0.489989   \n",
      "\n",
      "            AA_Return  AA_Volatility     AA_RSI   AA_MACD  AA_MACD_Signal  \\\n",
      "Date                                                                        \n",
      "2002-02-20   0.038581       0.377003  57.389682  0.345131       -0.203684   \n",
      "2002-02-21   0.023764       0.382840  61.223868  0.600317       -0.042884   \n",
      "2002-02-22   0.008538       0.371582  62.558369  0.840655        0.133824   \n",
      "2002-02-25   0.006085       0.371568  63.529889  1.053333        0.317726   \n",
      "2002-02-26  -0.013936       0.376289  59.686618  1.129705        0.480122   \n",
      "\n",
      "            ...  ZNH_Return  ZNH_Volatility    ZNH_RSI  ZNH_MACD  \\\n",
      "Date        ...                                                    \n",
      "2002-02-20  ...    0.032530        0.385721  60.914804  0.235341   \n",
      "2002-02-21  ...   -0.008751        0.388071  58.843239  0.227365   \n",
      "2002-02-22  ...   -0.002943        0.388680  58.133537  0.216327   \n",
      "2002-02-25  ...   -0.024203        0.401311  52.538063  0.187188   \n",
      "2002-02-26  ...    0.004235        0.363275  53.363408  0.165302   \n",
      "\n",
      "            ZNH_MACD_Signal  ZTR_Return  ZTR_Volatility    ZTR_RSI  ZTR_MACD  \\\n",
      "Date                                                                           \n",
      "2002-02-20         0.203659   -0.002780        0.052192  48.709934  0.000339   \n",
      "2002-02-21         0.208400   -0.002790        0.051131  45.654919 -0.002610   \n",
      "2002-02-22         0.209985   -0.001399        0.050758  44.163127 -0.005312   \n",
      "2002-02-25         0.205426    0.007002        0.054812  52.513959 -0.005262   \n",
      "2002-02-26         0.197401   -0.002780        0.053826  49.337035 -0.006005   \n",
      "\n",
      "            ZTR_MACD_Signal  \n",
      "Date                         \n",
      "2002-02-20         0.009727  \n",
      "2002-02-21         0.007260  \n",
      "2002-02-22         0.004745  \n",
      "2002-02-25         0.002744  \n",
      "2002-02-26         0.000994  \n",
      "\n",
      "[5 rows x 12505 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import talib  # Import TA-Lib\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Step 6: Feature Engineering (Using TA-Lib)\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "# Function to calculate RSI using TA-Lib\n",
    "def calculate_rsi(prices, period=14):\n",
    "    \"\"\"\n",
    "    Calculate the Relative Strength Index (RSI) using TA-Lib.\n",
    "    \"\"\"\n",
    "    return talib.RSI(prices, timeperiod=period)\n",
    "\n",
    "# Function to calculate MACD using TA-Lib\n",
    "def calculate_macd(prices, short_period=12, long_period=26, signal_period=9):\n",
    "    \"\"\"\n",
    "    Calculate the MACD and Signal Line using TA-Lib.\n",
    "    \"\"\"\n",
    "    macd, signal, _ = talib.MACD(prices, fastperiod=short_period, slowperiod=long_period, signalperiod=signal_period)\n",
    "    return macd, signal\n",
    "\n",
    "# Initialize a dictionary to store features for all stocks\n",
    "all_features = {}\n",
    "\n",
    "# Loop through each stock to calculate features\n",
    "for stock in adjusted_close_prices.columns:\n",
    "    prices = adjusted_close_prices[stock]\n",
    "    \n",
    "    # Calculate daily returns\n",
    "    returns = prices.pct_change().dropna()\n",
    "    \n",
    "    # Calculate rolling volatility (annualized)\n",
    "    volatility = returns.rolling(window=21).std() * np.sqrt(252)\n",
    "    \n",
    "    # Calculate RSI using TA-Lib\n",
    "    rsi = calculate_rsi(prices, period=14)\n",
    "    \n",
    "    # Calculate MACD and Signal Line using TA-Lib\n",
    "    macd, signal = calculate_macd(prices, short_period=12, long_period=26, signal_period=9)\n",
    "    \n",
    "    # Combine features for this stock into a DataFrame\n",
    "    stock_features = pd.DataFrame({\n",
    "        f'{stock}_Return': returns,\n",
    "        f'{stock}_Volatility': volatility,\n",
    "        f'{stock}_RSI': rsi,\n",
    "        f'{stock}_MACD': macd,\n",
    "        f'{stock}_MACD_Signal': signal\n",
    "    })\n",
    "    \n",
    "    # Add the stock's features to the dictionary\n",
    "    all_features[stock] = stock_features\n",
    "\n",
    "# Combine features for all stocks into a single DataFrame\n",
    "features = pd.concat(all_features.values(), axis=1).dropna()\n",
    "\n",
    "# Display the features DataFrame\n",
    "print(\"\\nEngineered Features Shape:\", features.shape)\n",
    "print(\"\\nEngineered Features Preview:\")\n",
    "print(features.head())\n",
    "\n",
    "# Optional: Save the features DataFrame to a new file\n",
    "features.to_csv('engineered_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4a35f96-be61-4c4b-a4d8-03398a3f7bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the index a DatetimeIndex? True\n",
      "Missing dates in the index: DatetimeIndex([], dtype='datetime64[ns]', name='Date', freq='B')\n",
      "Missing dates after filling: DatetimeIndex([], dtype='datetime64[ns]', name='Date', freq='B')\n",
      "Frequency of the index: <BusinessDay>\n"
     ]
    }
   ],
   "source": [
    "# Check if the index is a DatetimeIndex\n",
    "print(\"Is the index a DatetimeIndex?\", isinstance(adjusted_close_prices.index, pd.DatetimeIndex))\n",
    "\n",
    "# Check for missing dates\n",
    "print(\"Missing dates in the index:\", adjusted_close_prices.index[adjusted_close_prices.index.isnull()])\n",
    "# Fill missing dates and forward-fill missing values\n",
    "adjusted_close_prices = adjusted_close_prices.asfreq('B').ffill()\n",
    "\n",
    "# Verify that there are no missing dates\n",
    "print(\"Missing dates after filling:\", adjusted_close_prices.index[adjusted_close_prices.index.isnull()])\n",
    "# Infer and set the frequency explicitly\n",
    "freq = pd.infer_freq(adjusted_close_prices.index)\n",
    "if freq is None:\n",
    "    freq = 'B'  # Default to business days if frequency cannot be inferred\n",
    "adjusted_close_prices.index.freq = freq\n",
    "\n",
    "# Verify the frequency\n",
    "print(\"Frequency of the index:\", adjusted_close_prices.index.freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece75d68-408f-47d2-a804-4081b08471cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking stationarity for A:\n",
      "ADF Statistic: -0.18599325917636922\n",
      "p-value: 0.940170719549955\n",
      "Critical Values:\n",
      "   1%: -3.431731221687337\n",
      "   5%: -2.862150343170579\n",
      "   10%: -2.5670948876908053\n",
      "The series is non-stationary.\n",
      "Checking stationarity after differencing for A:\n",
      "ADF Statistic: -14.718258256809422\n",
      "p-value: 2.7779183210914663e-27\n",
      "Critical Values:\n",
      "   1%: -3.431731221687337\n",
      "   5%: -2.862150343170579\n",
      "   10%: -2.5670948876908053\n",
      "The series is stationary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "\n",
    "# Suppress ARIMA warnings\n",
    "'''warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"statsmodels\")\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"statsmodels\")'''\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Step 1: Check and Make Data Stationary\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "# Function to check stationarity\n",
    "def check_stationarity(prices):\n",
    "    \"\"\"\n",
    "    Perform the Augmented Dickey-Fuller test to check for stationarity.\n",
    "    \"\"\"\n",
    "    result = adfuller(prices.dropna())\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'   {key}: {value}')\n",
    "    if result[1] <= 0.05:\n",
    "        print(\"The series is stationary.\")\n",
    "    else:\n",
    "        print(\"The series is non-stationary.\")\n",
    "\n",
    "# Function to make data stationary\n",
    "def make_stationary(prices):\n",
    "    \"\"\"\n",
    "    Difference the prices to make the series stationary.\n",
    "    \"\"\"\n",
    "    return prices.diff().dropna()\n",
    "\n",
    "# Check and make data stationary for a sample stock\n",
    "sample_stock = adjusted_close_prices.columns[0]\n",
    "print(f\"Checking stationarity for {sample_stock}:\")\n",
    "check_stationarity(adjusted_close_prices[sample_stock])\n",
    "\n",
    "stationary_prices = make_stationary(adjusted_close_prices[sample_stock])\n",
    "print(f\"Checking stationarity after differencing for {sample_stock}:\")\n",
    "check_stationarity(stationary_prices)\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Step 2: Fit ARIMA to Each Stock and Extract Residuals\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "# Function to fit ARIMA and extract residuals\n",
    "def fit_arima_and_get_residuals(prices, order=(1, 1, 0)):\n",
    "    \"\"\"\n",
    "    Fit an ARIMA model to the prices and return the residuals.\n",
    "    \"\"\"\n",
    "    model = ARIMA(prices, order=order, dates=prices.index)\n",
    "    results = model.fit(start_params=[0.1, 0.1, 0.1])  # Custom starting parameters\n",
    "    residuals = results.resid\n",
    "    return residuals\n",
    "\n",
    "# Dictionary to store residuals for each stock\n",
    "residuals_dict = {}\n",
    "\n",
    "# Loop through each stock to fit ARIMA and extract residuals\n",
    "for stock in adjusted_close_prices.columns:\n",
    "    prices = adjusted_close_prices[stock].dropna()\n",
    "    stationary_prices = make_stationary(prices)\n",
    "    residuals = fit_arima_and_get_residuals(stationary_prices, order=(1, 1, 0))\n",
    "    residuals_dict[stock] = residuals\n",
    "\n",
    "# Convert residuals to a DataFrame\n",
    "residuals_df = pd.DataFrame(residuals_dict)\n",
    "\n",
    "# Display the residuals DataFrame\n",
    "print(\"\\nResiduals Shape:\", residuals_df.shape)\n",
    "print(\"\\nResiduals Preview:\")\n",
    "print(residuals_df.head())\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Step 3: Use Residuals as Input to LSTM\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "# Normalize the residuals\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "residuals_scaled = scaler.fit_transform(residuals_df)\n",
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, sequence_length=60):\n",
    "    \"\"\"\n",
    "    Create sequences of data for LSTM input.\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X_seq.append(data[i:i+sequence_length])\n",
    "        y_seq.append(data[i+sequence_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "sequence_length = 60  # Use 60 days of historical data to predict the next day\n",
    "X_seq, y_seq = create_sequences(residuals_scaled, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "split = int(0.8 * len(X_seq))\n",
    "X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_train.shape[1]))  # Output layer with one unit per stock\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the LSTM model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test.reshape(-1), y_pred.reshape(-1))\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Inverse transform the predictions to get actual residuals\n",
    "predicted_residuals = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# Display the predicted residuals\n",
    "print(\"\\nPredicted Residuals Preview:\")\n",
    "print(predicted_residuals[:5])\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Step 4: Combine ARIMA and LSTM Predictions\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "# Function to get ARIMA predictions\n",
    "def get_arima_predictions(prices, order=(1, 1, 0)):\n",
    "    \"\"\"\n",
    "    Get ARIMA predictions for the prices.\n",
    "    \"\"\"\n",
    "    model = ARIMA(prices, order=order, dates=prices.index)\n",
    "    results = model.fit(start_params=[0.1, 0.1, 0.1])  # Custom starting parameters\n",
    "    predictions = results.predict(start=0, end=len(prices)-1)\n",
    "    return predictions\n",
    "\n",
    "# Dictionary to store final predictions for each stock\n",
    "final_predictions_dict = {}\n",
    "\n",
    "# Loop through each stock to combine ARIMA and LSTM predictions\n",
    "for i, stock in enumerate(adjusted_close_prices.columns):\n",
    "    prices = adjusted_close_prices[stock].dropna()\n",
    "    stationary_prices = make_stationary(prices)\n",
    "    \n",
    "    # Get ARIMA predictions\n",
    "    arima_predictions = get_arima_predictions(stationary_prices, order=(1, 1, 0))\n",
    "    \n",
    "    # Get LSTM predictions for residuals\n",
    "    lstm_predictions = predicted_residuals[:, i]\n",
    "    \n",
    "    # Combine ARIMA and LSTM predictions\n",
    "    final_predictions = arima_predictions + lstm_predictions\n",
    "    \n",
    "    # Store the final predictions\n",
    "    final_predictions_dict[stock] = final_predictions\n",
    "\n",
    "# Convert final predictions to a DataFrame\n",
    "final_predictions_df = pd.DataFrame(final_predictions_dict, index=residuals_df.index[split+sequence_length:])\n",
    "\n",
    "# Display the final predictions\n",
    "print(\"\\nFinal Predictions Shape:\", final_predictions_df.shape)\n",
    "print(\"\\nFinal Predictions Preview:\")\n",
    "print(final_predictions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf8e986-ab18-4281-9309-10f114665ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
